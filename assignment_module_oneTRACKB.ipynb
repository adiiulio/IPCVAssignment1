{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNBgGYg_lpVN"
      },
      "source": [
        "# **Product Recognition of Food Products**\n",
        "\n",
        "## Image Processing and Computer Vision - Assignment Module \\#1\n",
        "\n",
        "\n",
        "Contacts:\n",
        "\n",
        "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
        "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
        "- Alex Costanzino -> alex.costanzino@unibo.it\n",
        "- Francesco Ballerini -> francesco.ballerini4@unibo.it\n",
        "\n",
        "\n",
        "Computer vision-based object detection techniques can be applied in super market settings to build a system that can identify products on store shelves.\n",
        "An example of how this system could be used would be to assist visually impaired customers or automate common store management tasks like detecting low-stock or misplaced products, given an image of a shelf in a store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW42NlZsyTv0"
      },
      "source": [
        "## Task\n",
        "Develop a computer vision system that, given a reference image for each product, is able to identify such product from one picture of a store shelf.\n",
        "\n",
        "<figure>\n",
        "<a href=\"https://imgbb.com/\">\n",
        "  <center>\n",
        "  <img src=\"https://i.ibb.co/TwkMWnH/Screenshot-2024-04-04-at-14-54-51.png\" alt=\"Screenshot-2024-04-04-at-14-54-51\" border=\"0\" width=\"300\" />\n",
        "</a>\n",
        "</figure>\n",
        "\n",
        "For each type of product displayed in the\n",
        "shelf the system should report:\n",
        "1. Number of instances;\n",
        "1. Dimension of each instance (width and height in pixel of the bounding box that enclose them);\n",
        "1. Position in the image reference system of each instance (center of the bounding box that enclose them).\n",
        "\n",
        "#### Example of expected output\n",
        "```\n",
        "Product 0 - 2 instance found:\n",
        "  Instance 1 {position: (256, 328), width: 57px, height: 80px}\n",
        "  Instance 2 {position: (311, 328), width: 57px, height: 80px}\n",
        "Product 1 â€“ 1 instance found:\n",
        ".\n",
        ".\n",
        ".\n",
        "```\n",
        "\n",
        "### Track A - Single Instance Detection\n",
        "Develop an object detection system to identify single instance of products given one reference image for each item and a scene image.\n",
        "\n",
        "The system should be able to correctly identify all the product in the shelves\n",
        "image.\n",
        "\n",
        "### Track B - Multiple Instances Detection\n",
        "In addition to what achieved at step A, the system should also be able to detect multiple instances of the same product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fIbZJKq16ba"
      },
      "source": [
        "## Data\n",
        "Two folders of images are provided:\n",
        "* **Models**: contains one reference image for each product that the system should be able to identify.\n",
        "* **Scenes**: contains different shelve pictures to test the developed algorithm in different scenarios. The images contained in this folder are corrupted by noise.\n",
        "\n",
        "#### Track A - Single Instance Detection\n",
        "* **Models**: {ref1.png to ref14.png}.\n",
        "* **Scenes**: {scene1.png to scene5.png}.\n",
        "\n",
        "#### Track B - Multiple Instances Detection\n",
        "* **Models**: {ref15.png to ref27.png}.\n",
        "* **Scenes**: {scene6.png to scene12.png}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NjP3GCdujYlw"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !cp -r /content/drive/MyDrive/AssignmentsIPCV/dataset.zip ./\n",
        "# !unzip dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KRBeGbKsEDe"
      },
      "source": [
        "## Evaluation criteria\n",
        "1. **Procedural correctness**. There are several ways to solve the assignment. Design your own sound approach and justify every decision you make;\n",
        "\n",
        "2. **Clarity and conciseness**. Present your work in a readable way: format your code and comment every important step;\n",
        "\n",
        "3. **Correctness of results**. Try to solve as many instances as possible. You should be able to solve all the instances of the assignment, however, a thoroughly justified and sound procedure with a lower number of solved instances will be valued **more** than a poorly designed approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.filters import unsharp_mask\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function shows the denoising processes, and the various attempts that have been made to understand which of the various methods produced the best output. In this case, the returned value is dst_median7, which is the denoising that first applies a median filter with ksize = 7, and afterwarts the Non-Linear Means filter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_denoising(img, k_size, sigma, show):\n",
        "\n",
        "    img = cv2.imread(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    #median filter, then mean and finally bilateral\n",
        "    median = cv2.medianBlur(img, 5)\n",
        "    means_and_median = cv2.fastNlMeansDenoisingColored(median,None,10,10,7,21)\n",
        "    means_and_median_and_bilateral = cv2.bilateralFilter(means_and_median, 5, 150, 75)\n",
        "\n",
        "    #median filter with parameter 7, followed by bilateral\n",
        "    median_7 = cv2.medianBlur(img, 7)\n",
        "    bilateral_median7 = cv2.bilateralFilter(median_7, 9, 255, 255)\n",
        "\n",
        "    #median filter with parameter 7, followed by Non Linear Means\n",
        "    dst_median7 = cv2.fastNlMeansDenoisingColored(median_7,None,10,10,7,21)\n",
        "\n",
        "    #cv2.bilateralFilter(img, filter_size, sigmaColor, sigmaSpace)\n",
        "    bilateral = cv2.bilateralFilter(img, 5, 200, 75)\n",
        "\n",
        "    #Now I want to try means and bilateral, as suggested\n",
        "    means = cv2.fastNlMeansDenoisingColored(img,None,10,10,7,21)\n",
        "    bilateral_means = cv2.bilateralFilter(means, 9, 150, 75)\n",
        "\n",
        "    \n",
        "    \n",
        "    sharpening_kernel = np.array([[0, -1, 0],\n",
        "                              [-1, 5, -1],\n",
        "                              [0, -1, 0]])\n",
        "    sharpened_image = cv2.filter2D(dst_median7, -1, sharpening_kernel)\n",
        "\n",
        "\n",
        "    if show:\n",
        "        fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 8))\n",
        "        \n",
        "        # Plot each image in its corresponding subplot\n",
        "        axes[0, 0].imshow(img)\n",
        "        axes[0, 0].set_title('Original')\n",
        "        axes[0, 1].imshow(bilateral)\n",
        "        axes[0, 1].set_title('bilateral and means')\n",
        "        axes[1, 0].imshow(bilateral_means)\n",
        "        axes[1, 0].set_title('bilateral_median and gauss with sharpening')\n",
        "        axes[1, 1].imshow(means_and_median_and_bilateral)\n",
        "        axes[1, 1].set_title('tutti i filtri')\n",
        "        axes[0, 2].imshow(bilateral_median7)\n",
        "        axes[0, 2].set_title('bilateral_median7')\n",
        "        axes[1, 2].imshow(dst_median7)\n",
        "        axes[1, 2].set_title('dst_median7')\n",
        "        plt.savefig('./dataset/denoising.png')\n",
        "        plt.show()\n",
        "    return dst_median7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def medianFilterMarginal_kri(img_path, filterSize, iterations, bil_ker):\n",
        "\n",
        "    img= cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img_index= extract_number(img_path)\n",
        "\n",
        "    imgHeight, imgWidth, channels = img.shape\n",
        "    outputImg = img.copy()\n",
        "\n",
        "    filterHeight = filterWidth = filterSize\n",
        "    filterEdge = filterWidth // 2\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        newOutputImg = np.zeros((imgHeight, imgWidth, channels), dtype=np.uint8)\n",
        "        for i in range(filterEdge, imgHeight - filterEdge):\n",
        "            for j in range(filterEdge, imgWidth - filterEdge):\n",
        "                # Get a frame around the pixel by the filter size\n",
        "                imgFilter = outputImg[i - filterEdge : i + filterEdge + 1, j - filterEdge : j + filterEdge + 1]\n",
        "\n",
        "                # Separate channels, reshape to a one-dimensional array and sort it\n",
        "                red = np.sort(imgFilter[:, :, 0].reshape(filterHeight * filterWidth))\n",
        "                green = np.sort(imgFilter[:, :, 1].reshape(filterHeight * filterWidth))\n",
        "                blue = np.sort(imgFilter[:, :, 2].reshape(filterHeight * filterWidth))\n",
        "\n",
        "                # Get the median intensity\n",
        "                newOutputImg[i][j][0] = red[(filterWidth * filterHeight) // 2]\n",
        "                newOutputImg[i][j][1] = green[(filterWidth * filterHeight) // 2]\n",
        "                newOutputImg[i][j][2] = blue[(filterWidth * filterHeight) // 2]\n",
        "        \n",
        "        outputImg = newOutputImg\n",
        "\n",
        "    median = cv2.bilateralFilter(outputImg, bil_ker, 60, 50)\n",
        "\n",
        "    img_write = cv2.cvtColor(median, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(f'./dataset/denoised_scenes/scene{img_index}.png', img_write)\n",
        "\n",
        "    print(f\"Results saved for {img_path} as ./dataset/denoised_scenes/scene{img_index}.png\")\n",
        "    \n",
        "    return outputImg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scene1_dn = show_denoising('./dataset/scenes/scene4.png', 9, 3, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_number(string):\n",
        "    # Using regex to find numbers in the string\n",
        "    numbers = re.findall(r'\\d+', string)\n",
        "    # Returning the first number found (if any)\n",
        "    return int(numbers[0]) if numbers else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denoise_scenes(img_path, kind):\n",
        "    '''\n",
        "    Takes a single image path as an input and executes the denoising according to the desired method.\n",
        "    Once that's done it saves the results in the denoised_scenes folder.\n",
        "    '''\n",
        "    #####BEFORE RUNNING THIS CREATE A 'denoised_scenes' folder inside 'dataset'\n",
        "    img= cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img_index= extract_number(img_path)\n",
        "\n",
        "    median = cv2.medianBlur(img, 5)\n",
        "    if kind=='dst':\n",
        "        img = cv2.fastNlMeansDenoisingColored(median,None,10,10,7,21)\n",
        "    elif kind=='bilateral':\n",
        "        img = cv2.bilateralFilter(median, 5, 150, 75)\n",
        "    elif kind == 'tutti':\n",
        "        means_and_median = cv2.fastNlMeansDenoisingColored(median,None,10,10,7,21)\n",
        "        img = cv2.bilateralFilter(means_and_median, 5, 150, 75)\n",
        "\n",
        "    img_write = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    cv2.imwrite(f'./dataset/denoised_scenes/scene{img_index}.png', img_write)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating auxiliary arrays to browse the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenes_path = './dataset/scenes/scene'\n",
        "\n",
        "scene_idx_start_a = 1\n",
        "scene_idx_end_a = 5\n",
        "scene_idx_start_b = 6\n",
        "scene_idx_end_b = 12\n",
        "\n",
        "scenes_a = [f\"{scenes_path}{i}.png\" for i in range(scene_idx_start_a, scene_idx_end_a + 1)]\n",
        "scenes_b = [f\"{scenes_path}{i}.png\" for i in range(scene_idx_start_b, scene_idx_end_b + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "dn_scenes_path = './dataset/denoised_scenes/scene'\n",
        "\n",
        "dn_scenes_a = [f\"{dn_scenes_path}{i}.png\" for i in range(scene_idx_start_a, scene_idx_end_a + 1)]\n",
        "dn_scenes_b = [f\"{dn_scenes_path}{i}.png\" for i in range(scene_idx_start_b, scene_idx_end_b + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_path = './dataset/models/ref'\n",
        "\n",
        "start_index_a = 1\n",
        "end_index_a = 14\n",
        "start_index_b = 15\n",
        "end_index_b = 27\n",
        "\n",
        "# Generate the array of file paths\n",
        "objs_a = [f\"{models_path}{i}.png\" for i in range(start_index_a, end_index_a + 1)]\n",
        "objs_b = [f\"{models_path}{i}.png\" for i in range(start_index_b, end_index_b + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Denoise Scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for scene in scenes_a:\n",
        "    denoise_scenes(scene, 'tutti')\n",
        "for scene in scenes_b:\n",
        "    denoise_scenes(scene, 'tutti')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved for ./dataset/scenes/scene6.png as ./dataset/denoised_scenes/scene6.png\n",
            "Results saved for ./dataset/scenes/scene7.png as ./dataset/denoised_scenes/scene7.png\n",
            "Results saved for ./dataset/scenes/scene8.png as ./dataset/denoised_scenes/scene8.png\n",
            "Results saved for ./dataset/scenes/scene9.png as ./dataset/denoised_scenes/scene9.png\n",
            "Results saved for ./dataset/scenes/scene10.png as ./dataset/denoised_scenes/scene10.png\n",
            "Results saved for ./dataset/scenes/scene11.png as ./dataset/denoised_scenes/scene11.png\n",
            "Results saved for ./dataset/scenes/scene12.png as ./dataset/denoised_scenes/scene12.png\n"
          ]
        }
      ],
      "source": [
        "filter_size = 5  # Increase the filter size for a stronger effect\n",
        "iterations = 2\n",
        "bil_ker=100\n",
        "for scene in scenes_b:\n",
        "    medianFilterMarginal_kri(scene, filter_size, iterations,bil_ker )\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Track A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Auxiliary function to manage each step of the recognition more easily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_output(prod_instances):\n",
        "    '''\n",
        "    Function which take every instance of a match between products and scene and \n",
        "    prints them in the required format\n",
        "    '''\n",
        "    \n",
        "    for product in prod_instances:\n",
        "        c = 1\n",
        "        print(f\"Product: {product['name']} - {product['count']} instance found\")\n",
        "        for instance in product['instances']:\n",
        "            print(f'\\t Instance {c} - position: {instance[2]}, width: {instance[1]} px, height:{instance[0]}px, scene:{instance[3]}')\n",
        "            c = c+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_box(img_path, dst):\n",
        "    ''''\n",
        "    Show the bounding box found after the recognition\n",
        "    '''\n",
        "    \n",
        "    img_ob = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img_ob, cv2.COLOR_BGR2RGB)\n",
        "    img_train_p = cv2.polylines(img,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
        "    plt.imshow(img_train_p, 'gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def siftDetect(img_path):\n",
        "    '''\n",
        "    Initialization of Sift algorithm, instatiation of images as cv2 objs\n",
        "    and calculation of kp and descriptors\n",
        "    '''\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    img_ob = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img_ob, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    #this is the computing of the keypoints and descriptors\n",
        "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
        "\n",
        "    return keypoints, descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flannMatch(img_ref_path, img_scene_path):\n",
        "    '''\n",
        "    Initialization of the FLANN matcher, to match kp from the 2 images\n",
        "    '''\n",
        "\n",
        "    FLANN_INDEX_KDTREE = 1\n",
        "\n",
        "    # Defining parameters for algorithm\n",
        "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
        "\n",
        "    # Defining search params.\n",
        "    # checks=50 specifies the number of times the trees in the index should be recursively traversed.\n",
        "    # Higher values gives better precision, but also takes more time\n",
        "    search_params = dict(checks = 50)\n",
        "\n",
        "    # Initializing matcher\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "    kp_ref, des_ref = siftDetect(img_ref_path)\n",
        "    kp_scene, des_scene = siftDetect(img_scene_path)\n",
        "\n",
        "    # Matching and finding the 2 closest elements for each query descriptor.\n",
        "    matches = flann.knnMatch(des_ref,des_scene,k=2)\n",
        "    \n",
        "    return matches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loweFilter(matches):\n",
        "    '''\n",
        "    Filter to distinguish valuable matches trough a treshold\n",
        "    '''\n",
        "    good = []\n",
        "    for m,n in matches:\n",
        "        if m.distance < 0.7*n.distance:\n",
        "            good.append(m)\n",
        "    return good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def homography(good, img_ref_path, img_scene_path, treshold):\n",
        "\n",
        "    #TODO switch to a dynamic tresholding mechanism\n",
        "\n",
        "    img_ob = cv2.imread(img_ref_path)\n",
        "    img_ref = cv2.cvtColor(img_ob, cv2.COLOR_RGB2BGR)\n",
        "    \n",
        "    if len(good) > treshold:\n",
        "        print(f'{img_ref_path} present in {img_scene_path}')\n",
        "        kp_ref, des_ref = siftDetect(img_ref_path)\n",
        "        kp_scene, des_scene = siftDetect(img_scene_path)\n",
        "        # building the correspondences arrays of good matches\n",
        "        src_pts = np.float32([kp_ref[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
        "        dst_pts = np.float32([kp_scene[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
        "        # Using RANSAC to estimate a robust homography.\n",
        "        # It returns the homography M and a mask for the discarded points\n",
        "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "\n",
        "        # Mask of discarded point used in visualization\n",
        "        matchesMask = mask.ravel().tolist()\n",
        "\n",
        "        # Corners of the query image\n",
        "        h, w, c = img_ref.shape\n",
        "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "        # Projecting the corners into the train image\n",
        "        dst = cv2.perspectiveTransform(pts, M)\n",
        "        \n",
        "        ret_array=[h,w, dst[0][0]]       \n",
        "        return ret_array\n",
        "    else:\n",
        "        #TODO: see if there is an alternative to this return\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Single Instance Matching Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_matching(img_ref_path, img_scene_path, treshold):\n",
        "    '''\n",
        "    Function which matches the obj image with the required scene. \n",
        "    It returns an array which is functional to format the output in the required style.\n",
        "    '''\n",
        "\n",
        "    #Finding kp, desc and all the matches\n",
        "    matches = flannMatch(img_ref_path, img_scene_path)\n",
        "    #filtering the matches trough a treshold\n",
        "    good = loweFilter(matches)\n",
        "    #TODO: we always pass the img path instead of cv2 obj, a bit inefficient\n",
        "    ret_array = homography(good, img_ref_path, img_scene_path, treshold)\n",
        "    \n",
        "    return ret_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def check_matching_old(img_obj_path, img_scene_path, treshold):\n",
        "    \n",
        "#     img_ob = cv2.imread(img_obj_path)\n",
        "#     img_obj = cv2.cvtColor(img_ob, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#     scene_ob = cv2.imread(img_scene_path)\n",
        "#     img_scene = cv2.cvtColor(scene_ob, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#     sift =cv2.SIFT_create()\n",
        "#     kp_query = sift.detect(img_obj)\n",
        "#     kp_train =sift.detect(img_scene)\n",
        "#     kp_query, des_query = sift.compute(img_obj, kp_query)\n",
        "#     kp_train, des_train = sift.compute(img_scene, kp_train)\n",
        "\n",
        "#     #treshold_percentage = len(kp_query) * treshold\n",
        "\n",
        "#     FLANN_INDEX_KDTREE = 1\n",
        "#     index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
        "#     search_params = dict(checks=50)\n",
        "\n",
        "#     flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "#     matches = flann.knnMatch(des_query, des_train, k=2)\n",
        "\n",
        "#     good = []\n",
        "#     for m, n in matches:\n",
        "#         if m.distance < 0.6 * n.distance:\n",
        "#             good.append(m)\n",
        "\n",
        "#     if len(good) > treshold:\n",
        "#         print(f'{img_obj_path} present in {img_scene_path}')\n",
        "        \n",
        "#         # building the correspondences arrays of good matches\n",
        "#         src_pts = np.float32([kp_query[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
        "#         dst_pts = np.float32([kp_train[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
        "#         # Using RANSAC to estimate a robust homography.\n",
        "#         # It returns the homography M and a mask for the discarded points\n",
        "#         M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "\n",
        "#         # Mask of discarded point used in visualization\n",
        "#         matchesMask = mask.ravel().tolist()\n",
        "\n",
        "#         # Corners of the query image\n",
        "#         h, w,c = img_obj.shape\n",
        "#         pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "#         # Projecting the corners into the train image\n",
        "#         dst = cv2.perspectiveTransform(pts, M)\n",
        "         \n",
        "#         ret_array=[h,w, dst[0][0]]       \n",
        "#         return ret_array\n",
        "#     else:\n",
        "#         #print(f'Not enough matches are found - {len(good)}/{treshold_percentage}')\n",
        "#         return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Execution examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./dataset/models/ref1.png present in ./dataset/denoised_scenes/scene1.png\n",
            "./dataset/models/ref1.png present in ./dataset/denoised_scenes/scene4.png\n",
            "./dataset/models/ref2.png present in ./dataset/denoised_scenes/scene1.png\n",
            "./dataset/models/ref3.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref4.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref5.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref8.png present in ./dataset/denoised_scenes/scene3.png\n",
            "./dataset/models/ref11.png present in ./dataset/denoised_scenes/scene5.png\n",
            "###########################\n"
          ]
        }
      ],
      "source": [
        "prod_instances=[]\n",
        "for obj in objs_a:\n",
        "    prod_description={}\n",
        "    obj_instances=[]\n",
        "    prod_description['name'] = extract_number(obj)\n",
        "    for scene in dn_scenes_a:\n",
        "        ret_array= check_matching(obj, scene, 80)\n",
        "        \n",
        "        \n",
        "        #heigth, width, pos = check_matching(obj, scene, 0.027)\n",
        "        if(len(ret_array)>0):\n",
        "            ret_array.append(extract_number(scene))\n",
        "            obj_instances.append(ret_array)\n",
        "            #print(ret_array)\n",
        "    prod_description['instances']=obj_instances    \n",
        "    prod_description['count'] = len(obj_instances)\n",
        "    prod_instances.append(prod_description)\n",
        "    \n",
        "print('###########################')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./dataset/models/ref1.png present in ./dataset/denoised_scenes/scene1.png\n",
            "./dataset/models/ref2.png present in ./dataset/denoised_scenes/scene1.png\n",
            "./dataset/models/ref5.png present in ./dataset/denoised_scenes/scene1.png\n",
            "./dataset/models/ref14.png present in ./dataset/denoised_scenes/scene1.png\n",
            "\n",
            "\n",
            "./dataset/models/ref2.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref3.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref4.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref5.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref8.png present in ./dataset/denoised_scenes/scene2.png\n",
            "./dataset/models/ref14.png present in ./dataset/denoised_scenes/scene2.png\n",
            "\n",
            "\n",
            "./dataset/models/ref3.png present in ./dataset/denoised_scenes/scene3.png\n",
            "./dataset/models/ref6.png present in ./dataset/denoised_scenes/scene3.png\n",
            "./dataset/models/ref7.png present in ./dataset/denoised_scenes/scene3.png\n",
            "./dataset/models/ref8.png present in ./dataset/denoised_scenes/scene3.png\n",
            "\n",
            "\n",
            "./dataset/models/ref1.png present in ./dataset/denoised_scenes/scene4.png\n",
            "./dataset/models/ref8.png present in ./dataset/denoised_scenes/scene4.png\n",
            "./dataset/models/ref9.png present in ./dataset/denoised_scenes/scene4.png\n",
            "\n",
            "\n",
            "./dataset/models/ref11.png present in ./dataset/denoised_scenes/scene5.png\n",
            "./dataset/models/ref12.png present in ./dataset/denoised_scenes/scene5.png\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Testing the matches for the first five scenes\n",
        "for scene in dn_scenes_a:\n",
        "    for obj in objs_a:\n",
        "        ret_array= check_matching(obj, scene, treshold=60)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_output(prod_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Track B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fillArea(img, dst):\n",
        "    # Ensure dst is a NumPy array\n",
        "    dst = np.array(dst, dtype=np.int32)\n",
        "    \n",
        "    # Reshape if needed (assuming dst is a flat list of points)\n",
        "    if dst.ndim == 1:\n",
        "        dst = dst.reshape((-1, 2))\n",
        "    \n",
        "    # Ensure dst has the shape (1, n, 2) for fillPoly\n",
        "    dst = dst.reshape((1, -1, 2))\n",
        "    \n",
        "    mask = np.zeros_like(img)\n",
        "    cv2.fillPoly(mask, dst, (255,))\n",
        "    img = cv2.add(img, mask)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./dataset/models/ref15.png present in ./dataset/denoised_scenes/scene6.png\n",
            "\n",
            "\n",
            "./dataset/models/ref16.png present in ./dataset/denoised_scenes/scene7.png\n",
            "\n",
            "\n",
            "./dataset/models/ref17.png present in ./dataset/denoised_scenes/scene8.png\n",
            "./dataset/models/ref23.png present in ./dataset/denoised_scenes/scene8.png\n",
            "./dataset/models/ref24.png present in ./dataset/denoised_scenes/scene8.png\n",
            "./dataset/models/ref26.png present in ./dataset/denoised_scenes/scene8.png\n",
            "\n",
            "\n",
            "./dataset/models/ref19.png present in ./dataset/denoised_scenes/scene9.png\n",
            "./dataset/models/ref20.png present in ./dataset/denoised_scenes/scene9.png\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "./dataset/models/ref17.png present in ./dataset/denoised_scenes/scene11.png\n",
            "./dataset/models/ref23.png present in ./dataset/denoised_scenes/scene11.png\n",
            "\n",
            "\n",
            "./dataset/models/ref26.png present in ./dataset/denoised_scenes/scene12.png\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for scene in dn_scenes_b:\n",
        "    for obj in objs_b:\n",
        "        ret_array= check_matching(img_ref_path=obj, img_scene_path=scene, treshold=40)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i = scene_idx_start_b\n",
        "# for scene in dn_scenes_b:\n",
        "#     #transform the path into a scene\n",
        "#     scene_ob = cv2.imread(scene)\n",
        "#     img_scene = cv2.cvtColor(scene_ob, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#     #now parameters we need\n",
        "#     dst_list = []\n",
        "#     object_info_list = []\n",
        "\n",
        "#     kp_scene, dsp_scene = siftDetect(scene)\n",
        "#     img_scene_c = img_scene.copy()\n",
        "#     img_scene_c_path = f'dataset/copied_images/scene{i}.png'\n",
        "#     img_write = cv2.cvtColor(img_scene_c, cv2.COLOR_RGB2BGR )\n",
        "#     cv2.imwrite(img_scene_c_path, img_write)\n",
        "    \n",
        "\n",
        "#     for ref in objs_b:\n",
        "#         img_ob = cv2.imread(ref)\n",
        "#         img_ref = cv2.cvtColor(img_ob, cv2.COLOR_RGB2BGR)\n",
        "#         while True:\n",
        "#             ret_array = check_matching(ref, scene)\n",
        "#             dst = ret_array[2]\n",
        "#             if dst is None:\n",
        "#                 break\n",
        "\n",
        "#             print(dst)\n",
        "#             dst_list.extend(np.int32(dst).flatten())\n",
        "#             print(dst_list)  # Output: [418, 165]\n",
        "#             img_scene_c = fillArea(img_scene_c, dst)\n",
        "#             img_scene_c = cv2.cvtColor(img_scene_c, cv2.COLOR_RGB2BGR)\n",
        "#             kp_scene, dsp_scene = siftDetect(img_scene_c_path)\n",
        "\n",
        "#     show_box(scene, dst_list)\n",
        "#     i = i+1\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for scene in dn_scenes_b:\n",
        "#     print(scene)\n",
        "#     for ref in objs_b:\n",
        "#         print(ref)\n",
        "#         img_rgb = cv2.imread(scene)\n",
        "#         assert img_rgb is not None, \"file could not be read, check with os.path.exists()\"\n",
        "#         img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "#         template = cv2.imread(ref)\n",
        "#         print(img_gray, )\n",
        "#         assert template is not None, \"file could not be read, check with os.path.exists()\"\n",
        "#         w, h, c = template.shape\n",
        "        \n",
        "#         res = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)\n",
        "#         if len(res)>0:\n",
        "#             threshold = 0.7\n",
        "#             loc = np.where( res >= threshold)\n",
        "#             for pt in zip(*loc[::-1]):\n",
        "#                 cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n",
        "#         else:\n",
        "#             print(\"[]\")\n",
        "\n",
        "#         idx = extract_number(scene) \n",
        "#         img_write = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "#         cv2.imwrite(f'dataset/copied_images/scene{idx}.png',img_rgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved for ./dataset/denoised_scenes/scene6.png as ./dataset/copied_images/res_scene6.png\n",
            "Results saved for ./dataset/denoised_scenes/scene7.png as ./dataset/copied_images/res_scene7.png\n",
            "Results saved for ./dataset/denoised_scenes/scene8.png as ./dataset/copied_images/res_scene8.png\n",
            "Results saved for ./dataset/denoised_scenes/scene9.png as ./dataset/copied_images/res_scene9.png\n",
            "Results saved for ./dataset/denoised_scenes/scene10.png as ./dataset/copied_images/res_scene10.png\n",
            "Results saved for ./dataset/denoised_scenes/scene11.png as ./dataset/copied_images/res_scene11.png\n",
            "Results saved for ./dataset/denoised_scenes/scene12.png as ./dataset/copied_images/res_scene12.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iterate over each scene\n",
        "for scene_path in dn_scenes_b:\n",
        "    img_rgb = cv2.imread(scene_path)\n",
        "    img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB)\n",
        "    assert img_rgb is not None, f\"Scene file {scene_path} could not be read, check with os.path.exists()\"\n",
        "\n",
        "    # Iterate over each template\n",
        "    for template_path in objs_b:\n",
        "\n",
        "        template_rgb = cv2.imread(template_path)\n",
        "        template_rgb = cv2.cvtColor(template_rgb, cv2.COLOR_BGR2RGB)\n",
        "        assert template_rgb is not None, f\"Template file {template_path} could not be read, check with os.path.exists()\"\n",
        "\n",
        "        # Ensure the template and image have the same number of channels\n",
        "        if template_rgb.shape[2] != img_rgb.shape[2]:\n",
        "            raise ValueError(\"The template and image must have the same number of channels\")\n",
        "\n",
        "        # Check if template is larger than the image\n",
        "        if template_rgb.shape[0] > img_rgb.shape[0] or template_rgb.shape[1] > img_rgb.shape[1]:\n",
        "            #print(f\"Skipping template {template_path} because it is larger than the scene {scene_path}\")\n",
        "            continue\n",
        "\n",
        "        # Perform template matching\n",
        "        res = cv2.matchTemplate(img_rgb, template_rgb, cv2.TM_CCOEFF_NORMED)\n",
        "        \n",
        "        # Define the threshold and find locations where matching results are above the threshold\n",
        "        threshold = 0.75\n",
        "        loc = np.where(res >= threshold)\n",
        "\n",
        "        # Draw rectangles around matched regions\n",
        "        for pt in zip(*loc[::-1]):\n",
        "            cv2.rectangle(img_rgb, pt, (pt[0] + template_rgb.shape[1], pt[1] + template_rgb.shape[0]), (0, 0, 255), 2)\n",
        "\n",
        "        \n",
        "\n",
        "    # Save the result for the current scene\n",
        "    result_path = f'./dataset/copied_images/res_{os.path.basename(scene_path)}'\n",
        "    img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(result_path, img_rgb)\n",
        "    print(f\"Results saved for {scene_path} as {result_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
